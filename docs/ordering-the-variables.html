<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Model Selection in High Dimensions</title>
  <meta name="description" content="This is the content of the course Model Selection. The output format for this project is bookdown::gitbook.">
  <meta name="generator" content="bookdown 0.5 and GitBook 2.6.7">

  <meta property="og:title" content="Model Selection in High Dimensions" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is the content of the course Model Selection. The output format for this project is bookdown::gitbook." />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Model Selection in High Dimensions" />
  
  <meta name="twitter:description" content="This is the content of the course Model Selection. The output format for this project is bookdown::gitbook." />
  

<meta name="author" content="Maria-Pia Victoria-Feser (professor), Cesare Miglioli and Guillaume Blanc (teaching assistants)">


<meta name="date" content="2018-02-18">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="assessing-the-validity-of-a-model.html">
<link rel="next" href="shinkage-methods.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#read-this-part-first"><i class="fa fa-check"></i><b>1.1</b> Read this part first</a></li>
<li class="chapter" data-level="1.2" data-path="introduction.html"><a href="introduction.html#content-choice-and-structure"><i class="fa fa-check"></i><b>1.2</b> Content choice and structure</a><ul>
<li class="chapter" data-level="1.2.1" data-path="introduction.html"><a href="introduction.html#bibliography"><i class="fa fa-check"></i><b>1.2.1</b> Bibliography</a></li>
<li class="chapter" data-level="1.2.2" data-path="introduction.html"><a href="introduction.html#useful-links"><i class="fa fa-check"></i><b>1.2.2</b> Useful links</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="introduction.html"><a href="introduction.html#using-r"><i class="fa fa-check"></i><b>1.3</b> Using R</a><ul>
<li class="chapter" data-level="1.3.1" data-path="introduction.html"><a href="introduction.html#useful-r-packages"><i class="fa fa-check"></i><b>1.3.1</b> Useful R packages</a></li>
<li class="chapter" data-level="1.3.2" data-path="introduction.html"><a href="introduction.html#managing-data"><i class="fa fa-check"></i><b>1.3.2</b> Managing Data</a></li>
<li class="chapter" data-level="1.3.3" data-path="introduction.html"><a href="introduction.html#loading-data-from-an-r-package"><i class="fa fa-check"></i><b>1.3.3</b> Loading data from an R package</a></li>
<li class="chapter" data-level="1.3.4" data-path="introduction.html"><a href="introduction.html#loading-data-from-a-local-file"><i class="fa fa-check"></i><b>1.3.4</b> Loading data from a local file</a></li>
<li class="chapter" data-level="1.3.5" data-path="introduction.html"><a href="introduction.html#loading-data-from-an-online-file"><i class="fa fa-check"></i><b>1.3.5</b> Loading data from an online file</a></li>
<li class="chapter" data-level="1.3.6" data-path="introduction.html"><a href="introduction.html#loading-data-from-an-online-database-using-a-mysql-query-optional"><i class="fa fa-check"></i><b>1.3.6</b> Loading data from an online database using a mySQL query (Optional)</a></li>
<li class="chapter" data-level="1.3.7" data-path="introduction.html"><a href="introduction.html#data-wrangling"><i class="fa fa-check"></i><b>1.3.7</b> Data Wrangling</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="introduction.html"><a href="introduction.html#writing-reports"><i class="fa fa-check"></i><b>1.4</b> Writing reports</a><ul>
<li class="chapter" data-level="1.4.1" data-path="introduction.html"><a href="introduction.html#r-markdown"><i class="fa fa-check"></i><b>1.4.1</b> R Markdown</a></li>
<li class="chapter" data-level="1.4.2" data-path="introduction.html"><a href="introduction.html#github"><i class="fa fa-check"></i><b>1.4.2</b> GitHub</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="introduction.html"><a href="introduction.html#examples"><i class="fa fa-check"></i><b>1.5</b> Examples</a><ul>
<li class="chapter" data-level="1.5.1" data-path="introduction.html"><a href="introduction.html#data-on-malnutrition-in-zambia"><i class="fa fa-check"></i><b>1.5.1</b> Data on Malnutrition in Zambia</a></li>
<li class="chapter" data-level="1.5.2" data-path="introduction.html"><a href="introduction.html#prognostic-factors-in-childhood-leukemia"><i class="fa fa-check"></i><b>1.5.2</b> Prognostic Factors in Childhood Leukemia</a></li>
<li class="chapter" data-level="1.5.3" data-path="introduction.html"><a href="introduction.html#r-package-quantmod"><i class="fa fa-check"></i><b>1.5.3</b> R package quantmod</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="introduction.html"><a href="introduction.html#fundamental-statistical-concepts"><i class="fa fa-check"></i><b>1.6</b> Fundamental statistical concepts</a><ul>
<li class="chapter" data-level="1.6.1" data-path="introduction.html"><a href="introduction.html#sample-and-population"><i class="fa fa-check"></i><b>1.6.1</b> Sample and population</a></li>
<li class="chapter" data-level="1.6.2" data-path="introduction.html"><a href="introduction.html#models-and-risk"><i class="fa fa-check"></i><b>1.6.2</b> Models and risk</a></li>
<li class="chapter" data-level="1.6.3" data-path="introduction.html"><a href="introduction.html#estimators-and-associated-variability"><i class="fa fa-check"></i><b>1.6.3</b> Estimators and associated variability</a></li>
<li class="chapter" data-level="1.6.4" data-path="introduction.html"><a href="introduction.html#simulating-the-population-using-resampling-techniques"><i class="fa fa-check"></i><b>1.6.4</b> Simulating the population using resampling techniques</a></li>
<li class="chapter" data-level="1.6.5" data-path="introduction.html"><a href="introduction.html#model-selection"><i class="fa fa-check"></i><b>1.6.5</b> Model Selection</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html"><i class="fa fa-check"></i><b>2</b> Assessing the validity of a model</a><ul>
<li class="chapter" data-level="2.1" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#introduction-1"><i class="fa fa-check"></i><b>2.1</b> Introduction</a></li>
<li class="chapter" data-level="2.2" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#cross-validation"><i class="fa fa-check"></i><b>2.2</b> Cross-Validation</a></li>
<li class="chapter" data-level="2.3" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#covariance-penalties-criteria"><i class="fa fa-check"></i><b>2.3</b> Covariance Penalties Criteria</a><ul>
<li class="chapter" data-level="2.3.1" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#introduction-2"><i class="fa fa-check"></i><b>2.3.1</b> Introduction</a></li>
<li class="chapter" data-level="2.3.2" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#mallows-c_p"><i class="fa fa-check"></i><b>2.3.2</b> Mallows <span class="math inline">\(C_p\)</span></a></li>
<li class="chapter" data-level="2.3.3" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#efrons-q-class"><i class="fa fa-check"></i><b>2.3.3</b> Efron’s <span class="math inline">\(q\)</span>-class</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#information-theory-and-bayesian-criteria"><i class="fa fa-check"></i><b>2.4</b> Information Theory and Bayesian Criteria</a><ul>
<li class="chapter" data-level="2.4.1" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#aic-akaike-information-criterion"><i class="fa fa-check"></i><b>2.4.1</b> AIC: Akaike Information Criterion</a></li>
<li class="chapter" data-level="2.4.2" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#bic-bayesian-information-criterion"><i class="fa fa-check"></i><b>2.4.2</b> BIC: Bayesian Information Criterion</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#mean-squared-error-based-criteria"><i class="fa fa-check"></i><b>2.5</b> Mean Squared Error Based Criteria</a><ul>
<li class="chapter" data-level="2.5.1" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#steins-unbiased-risk-estimator-sure"><i class="fa fa-check"></i><b>2.5.1</b> Stein’s unbiased risk estimator (SURE)</a></li>
<li class="chapter" data-level="2.5.2" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#the-focused-information-criterion-fic"><i class="fa fa-check"></i><b>2.5.2</b> The Focused Information Criterion (FIC)</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#classification-measures"><i class="fa fa-check"></i><b>2.6</b> Classification measures</a></li>
<li class="chapter" data-level="2.7" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#generalized-measures"><i class="fa fa-check"></i><b>2.7</b> Generalized measures</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="ordering-the-variables.html"><a href="ordering-the-variables.html"><i class="fa fa-check"></i><b>3</b> Ordering the variables</a><ul>
<li class="chapter" data-level="3.1" data-path="ordering-the-variables.html"><a href="ordering-the-variables.html#introduction-3"><i class="fa fa-check"></i><b>3.1</b> Introduction</a></li>
<li class="chapter" data-level="3.2" data-path="ordering-the-variables.html"><a href="ordering-the-variables.html#least-squares-stepwise-and-orthogonal-matching-pursuit"><i class="fa fa-check"></i><b>3.2</b> Least squares stepwise and orthogonal matching pursuit</a></li>
<li class="chapter" data-level="3.3" data-path="ordering-the-variables.html"><a href="ordering-the-variables.html#classification-and-regression-tree-cart-7"><i class="fa fa-check"></i><b>3.3</b> Classification And Regression Tree (CART)</a><ul>
<li class="chapter" data-level="3.3.1" data-path="ordering-the-variables.html"><a href="ordering-the-variables.html#regression-tree"><i class="fa fa-check"></i><b>3.3.1</b> Regression tree</a></li>
<li class="chapter" data-level="3.3.2" data-path="ordering-the-variables.html"><a href="ordering-the-variables.html#classification-trees"><i class="fa fa-check"></i><b>3.3.2</b> Classification Trees</a></li>
<li class="chapter" data-level="3.3.3" data-path="ordering-the-variables.html"><a href="ordering-the-variables.html#remarks"><i class="fa fa-check"></i><b>3.3.3</b> Remarks</a></li>
<li class="chapter" data-level="3.3.4" data-path="ordering-the-variables.html"><a href="ordering-the-variables.html#others"><i class="fa fa-check"></i><b>3.3.4</b> Others</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="shinkage-methods.html"><a href="shinkage-methods.html"><i class="fa fa-check"></i><b>4</b> Shinkage Methods</a><ul>
<li class="chapter" data-level="4.1" data-path="shinkage-methods.html"><a href="shinkage-methods.html#example-one"><i class="fa fa-check"></i><b>4.1</b> Example one</a></li>
<li class="chapter" data-level="4.2" data-path="shinkage-methods.html"><a href="shinkage-methods.html#example-two"><i class="fa fa-check"></i><b>4.2</b> Example two</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="smoothing-and-averaging.html"><a href="smoothing-and-averaging.html"><i class="fa fa-check"></i><b>5</b> Smoothing and averaging</a><ul>
<li class="chapter" data-level="5.1" data-path="smoothing-and-averaging.html"><a href="smoothing-and-averaging.html#bagging"><i class="fa fa-check"></i><b>5.1</b> Bagging</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="extensions.html"><a href="extensions.html"><i class="fa fa-check"></i><b>6</b> Extensions</a><ul>
<li class="chapter" data-level="6.1" data-path="extensions.html"><a href="extensions.html#pdc"><i class="fa fa-check"></i><b>6.1</b> PDC</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Model Selection in High Dimensions</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="ordering-the-variables" class="section level1">
<h1><span class="header-section-number">3</span> Ordering the variables</h1>
<div id="introduction-3" class="section level2">
<h2><span class="header-section-number">3.1</span> Introduction</h2>
<p>The criteria presented in Chapter 2 are out-of-sample measures of adequacy for a given model. Model selection consists in finding the <em>best</em> model (or a limited set), like the one optimizing the out-of-sample adequacy. This implies comparing the potential models that can be built given the available data.</p>
<p>In high dimensional settings, the available set of potential predictors can be very large and an exhaustive building of potential models is just impossible. One hence needs to find <em>suitable</em> methods for ordering the predictors that enter the model sequentially, to constitute, at most, <span class="math inline">\(\huge p\)</span> potential models to be assessed and compared.</p>
<p>In this chapter, we present some available ordering algorithms, which have the following form:</p>
<ol style="list-style-type: decimal">
<li>Let <span class="math inline">\(\huge \cal{M}_0\)</span> denote the null model, which typically contains no predictors.</li>
<li>For <span class="math inline">\(\huge k = 0,\ldots,p-1\)</span>, do :</li>
</ol>
<ol style="list-style-type: lower-alpha">
<li>Consider all <span class="math inline">\(\huge p-k\)</span> models that augment the predictors in <span class="math inline">\(\huge \mathcal{M}_k\)</span> with one additional predictor.</li>
<li>Choose the <em>best</em> among these <span class="math inline">\(\huge p-k\)</span> models, and call it <span class="math inline">\(\huge \mathcal{M}_{k+1}\)</span>.</li>
</ol>
<ol start="3" style="list-style-type: decimal">
<li>Stop the algorithm if <span class="math inline">\(\huge \mathcal{M}_{k+1}\)</span> is not better than <span class="math inline">\(\huge \mathcal{M}_{k}\)</span> and provide <span class="math inline">\(\huge \mathcal{M}_{k}\)</span> as output.</li>
</ol>
<p>These algorithms are generally named stepwise forward algorithms and they differ in the definition of <em>best</em> in 2(b) and in the stopping rule criteria in 3. For the latter, it is actually chosen among the criteria presented in Chapter 2.</p>
<blockquote>
<p>Exercise: Compare the number of models to be considered in an exhaustive approach to the ones considered in a stepwise forward approach. Choose <span class="math inline">\(\huge p=\{5,10,15,20\}\)</span> and suppose that the true (best) model has <span class="math inline">\(\huge p\)</span> and <span class="math inline">\(\huge p/2\)</span> predictors.</p>
</blockquote>
</div>
<div id="least-squares-stepwise-and-orthogonal-matching-pursuit" class="section level2">
<h2><span class="header-section-number">3.2</span> Least squares stepwise and orthogonal matching pursuit</h2>
</div>
<div id="classification-and-regression-tree-cart-7" class="section level2">
<h2><span class="header-section-number">3.3</span> Classification And Regression Tree (CART)<a href="#fn4" class="footnoteRef" id="fnref4"><sup>4</sup></a></h2>
<p>The regression model involves estimating the <em>conditional mean</em> of the response variable <span class="math inline">\(\huge Y\)</span> given a set of predictors <span class="math inline">\(\huge X_j, j=1,\ldots,p\)</span> (collected in the <span class="math inline">\(\huge n\times p\)</span> <em>design matrix</em> <span class="math inline">\(\huge \mathbf{X}\)</span>), i.e. <span class="math inline">\(\huge \mu(\mathbf{X})=\mathbf{X}\beta, \dim(\beta)=p\)</span>. The parameters <span class="math inline">\(\huge \beta\)</span> represent the slopes of the linear regression of <span class="math inline">\(\huge Y\)</span> on <span class="math inline">\(\huge \mathbf{X}\)</span>. The basis of regression trees is to approximate these slopes by partitionning the observed predictors into consecutive sets of values, for which the observed mean response is computed. More precisely, tree-based methods partition the predictors’ space (feature space) into a set of rectangles, and then fit a simple model (i.e. computes the mean) in each one.</p>
<p>The partitionning is done sequentially, with usually a binary partionning (i.e. in two consecutive parts), one predictor at the time. Figure  shows such a sequence in the case of two predictors <span class="math inline">\(\huge X_1, X_2\)</span>, with splitting values <span class="math inline">\(\huge t_m\)</span> and (splitted) regions <span class="math inline">\(\huge R_m, m=1,\ldots,M, M=5\)</span>. Note that the partition on the top left panel is not possible since one of the consequence of a recursive splitting is that each region is necessary included in only one larger region.</p>
<blockquote>
<p>Illustration of the tree method (taken from <span class="citation">(<span class="citeproc-not-found" data-reference-id="HaTiFr:09"><strong>???</strong></span>)</span>)<br />
<img src="Figures/RegTree.png" alt="Top" /></p>
</blockquote>
<p>A regression tree algorithm has the following features (at each stage of the algorithm) that are problem dependent:</p>
<ul>
<li>criterion for the choice of the splitting variable <span class="math inline">\(\huge X_j\)</span></li>
<li>criterion for the splitting value <span class="math inline">\(\huge t_m\)</span></li>
<li>the model that is fitted in each region <span class="math inline">\(\huge R_m\)</span></li>
<li>the size of the tree <span class="math inline">\(\huge M\)</span></li>
<li>the topology (shape) the tree should have.</li>
</ul>
<div id="regression-tree" class="section level3">
<h3><span class="header-section-number">3.3.1</span> Regression tree</h3>
In the linear regression model, since one is modelling a conditional mean, it makes sense to fit a constant model in each region <span class="math inline">\(\huge R_m\)</span>, so that the fitted model at the <em>mth</em> stage of the algoritm is
<span class="math display">\[\begin{equation}
\huge
\hat{\mu}(\mathbf{X})= \sum_{m=1}^M c_m(y_i\vert \mathbf{x}_i\in R_m)
\end{equation}\]</span>
with
<span class="math display">\[\begin{equation}
\huge
c_m(y_i\vert \mathbf{x}_i\in R_m)=\frac{1}{n_m}\sum_i^n y_i \iota (\mathbf{x}_i\in R_m), n_m=\sum_i^n \iota (\mathbf{x}_i\in R_m)
\end{equation}\]</span>
<p>so that the residual sum of squares (RSS) <span class="math inline">\(\huge \sum_{i=1}^n\left(y_i-\hat{\mu}(\mathbf{x}_i)\right)^2\)</span> is minimized.</p>
At stage <span class="math inline">\(\huge m\)</span> of the algorithm, two new regions will be created (actually one of the former region will be split into two parts), <span class="math inline">\(\huge R_{m_1}(t_m)\)</span> and <span class="math inline">\(\huge R_{m_2}(t_m)\)</span> according to a splitting scalar <span class="math inline">\(\huge t_m\)</span> so that the RSS is minimized, i.e.
<span class="math display">\[\begin{eqnarray}
\huge
\min_{j,t_m}&amp; \huge \left\{\sum_{\mathbf{x}_i \in R_{m_1}(t_m)} \left(y_i-c_m(y_i\vert \mathbf{x}_i\in R_{m_1}(t_m))\right)^2 \right. \\  &amp; \left.  \huge + 
\sum_{\mathbf{x}_i \in R_{m_2}(t_m)} \left(y_i-c_m(y_i\vert \mathbf{x}_i \in R_{m_2}(t_m))\right)^2\right\}
\end{eqnarray}\]</span>
<p>Hence, the splitting objective does only depend on the RSS of the two newly created regions (and not the others), which is a necessary simplification in order to avoid running into computationnally infeasible algorithm.</p>
For the size of the tree, there are several strategies. One approach would to stop splitting (or growing the tree) when the decrease in RSS (computed globally) due to the split is below some threshold. Another strategy is to set a priori the tree size <span class="math inline">\(\huge M\)</span> and then <em>prun</em> the tree using <em>cost-complexity pruning</em>. Let the subtree <span class="math inline">\(\huge T \subset T_M\)</span> of the tree <span class="math inline">\(\huge T_M\)</span> obtained by fixing the size to <span class="math inline">\(\huge M\)</span>, let also <span class="math inline">\(\huge \vert T\vert\)</span> be the size of <span class="math inline">\(\huge T\)</span> and <span class="math inline">\(\huge R_m(T), c_m(T)\)</span> defined accordingly as before, the cost complexity criterion is given by
<span class="math display">\[\begin{equation}
\huge
C_{\alpha}(T)=\sum_{m=1}^{\vert T\vert}\sum_{\mathbf{x}_i\in R_m(T)}\left(y_i-c_m(T)\right)^2 - \alpha \vert T\vert
\end{equation}\]</span>
<p>Hence, the idea is to find, for each <span class="math inline">\(\huge \alpha\)</span>, the (unique) subtree <span class="math inline">\(\huge T_{\alpha} \subset T_M\)</span> that minimizes <span class="math inline">\(\huge C_{\alpha}(T)\)</span>. The tuning parameter <span class="math inline">\(\huge \alpha \geq 0\)</span> governs the tradeoff between tree size (to avoid overfitting) and its goodness of fit to the data. Large values of <span class="math inline">\(\huge \alpha\)</span> result in smaller trees <span class="math inline">\(\huge T_{\alpha}\)</span> (<span class="math inline">\(\huge \alpha\)</span> corresponding to <span class="math inline">\(\huge T_M\)</span>). A sequence of subtrees can hence be constructed by letting <span class="math inline">\(\huge \alpha\)</span> increase, and stop when a criterion such as tenfold cross-validation is optimized.</p>
<blockquote>
<p>Exercise: play with the rpart package in R (see also <a href="https://www.statmethods.net/advstats/cart.html" class="uri">https://www.statmethods.net/advstats/cart.html</a>) and a suitable dataset.</p>
</blockquote>
</div>
<div id="classification-trees" class="section level3">
<h3><span class="header-section-number">3.3.2</span> Classification Trees</h3>
<p>If the target is a classification outcome taking values <span class="math inline">\(\huge k=1, 2,\ldots,K\)</span>, the criteria for splitting (growing) and pruning the tree need to be adapted. Let <span class="math inline">\(\huge p_{mk}=1/n_m\sum_{\mathbf{x}_i\in R_m}\iota(y_i= k)\)</span> be the proportion of of class <span class="math inline">\(\huge k\)</span> observations in region <span class="math inline">\(\huge m\)</span>. Different alternatives to the RSS (i.e. risk measures) exist and include the following:</p>
<ul>
<li><em>Misclassification error</em>: <span class="math inline">\(\huge 1-p_{mk}\)</span></li>
<li><em>Gini index</em>: <span class="math inline">\(\huge \sum_{k=1}^Kp_{mk}(1-p_{mk})= \sum_{k=1}^K\sum_{k&#39;\neq k}p_{mk}p_{mk&#39;}\)</span></li>
<li><em>Cross-entropy or deviance</em>: <span class="math inline">\(\huge - \sum_{k=1}^Kp_{mk}\log(p_{mk})\)</span></li>
</ul>
When <span class="math inline">\(\huge K=2\)</span>, one is in the binary case, which can be modelled as a <em>logistic regression</em>, i.e. <span class="math inline">\(\huge Y_i\vert\mathbf{x}_i\sim B(1,\pi(\mathbf{x}_i))\)</span>, with <span class="math inline">\(\huge \mbox{logit}(\pi(\mathbf{x}_i))=\mathbf{x}_i\beta, \dim(\beta)=p\)</span>, or
<span class="math display">\[\begin{equation}
\huge
\pi(\mathbf{x}_i)=\frac{\exp(\mathbf{x}_i\beta)}{1+\exp(\mathbf{x}_i\beta)}
\end{equation}\]</span>
<p>In that case, the deviance measure will correspond the the best fitted logistic regression.</p>
<blockquote>
<p>Figure: Put here a figure like FIGURE 9.3 in <span class="citation">(<span class="citeproc-not-found" data-reference-id="HaTiFr:09"><strong>???</strong></span>)</span> with the caption *Alternative risk measures in the binary case, as a function of the proportion <span class="math inline">\(\huge \pi\)</span>. The deviance has been scaled to pass through <span class="math inline">\(\huge (0.5, 0.5)\)</span>.</p>
</blockquote>
<p>The deviance and the Gini index have the advantage of being differentiable functions, and hence more amenable to numerical optimization.</p>
<blockquote>
<p>Exercise: write the CART algorithm (with pruniing) in the classification case.</p>
</blockquote>
<blockquote>
<p>Exercise: play with the rpart package in R (see also <a href="https://www.statmethods.net/advstats/cart.html" class="uri">https://www.statmethods.net/advstats/cart.html</a>) and a suitable dataset.</p>
</blockquote>
</div>
<div id="remarks" class="section level3">
<h3><span class="header-section-number">3.3.3</span> Remarks</h3>
<ul>
<li><p>CART methods provide prediction models <span class="math inline">\(\huge \hat{\mu}(\mathbf{X})\)</span>. They hence do not necessarily lead to parsimonious models, which, in terms of model interpretability, might be a serious problem. Are there ways around it?</p></li>
<li><p>In classification problems, the consequences of misclassifying observations are more serious in some classes than others. For example, it is probably worse to predict that a person will not have a heart attack when he/she actually will, than vice versa. To account for this, one can define a <span class="math inline">\(\huge K \times K\)</span> loss matrix <span class="math inline">\(\huge L\)</span>, with <span class="math inline">\(\huge L_{kk&#39;}\)</span> being the loss incurred for classifying a class <span class="math inline">\(\huge k\)</span> observation as class <span class="math inline">\(\huge k&#39;\)</span>, with obviously <span class="math inline">\(\huge L_{kk}=0, \forall k\)</span>. The Gini index could then be modified as <span class="math inline">\(\huge \sum_{k=1}^K\sum_{k&#39;\neq k}L_{kk&#39;}p_{mk}p_{mk&#39;}\)</span> which has only an effect for <span class="math inline">\(\huge K&gt;2\)</span>. In the binary case, a better approach is to directly weight the observations in class <span class="math inline">\(\huge k\)</span> by <span class="math inline">\(\huge L_{kk&#39;}\)</span> and use either the Gini index or the deviance.</p></li>
<li><p>One major problem with trees is their high variance. Often a small change in the data can result in a very different series of splits, making interpre- tation somewhat precarious. Averaging methods such as bagging (see Section ???) can be used to reduce the variability of the tree estimator.</p></li>
</ul>
</div>
<div id="others" class="section level3">
<h3><span class="header-section-number">3.3.4</span> Others</h3>
<ul>
<li>matching pursuit</li>
<li>etc.</li>
</ul>

</div>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="4">
<li id="fn4"><p>See also <span class="citation">(<span class="citeproc-not-found" data-reference-id="HaTiFr:09"><strong>???</strong></span>)</span>, chapter 9.<a href="ordering-the-variables.html#fnref4">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="assessing-the-validity-of-a-model.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="shinkage-methods.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
