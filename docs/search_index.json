[
["index.html", "Model Selection in High Dimensions 1 Introduction 1.1 Content choice and structure 1.2 Using R 1.3 Writing reports 1.4 Examples 1.5 Fundamental statistical concepts", " Model Selection in High Dimensions Maria-Pia Victoria-Feser (professor), Cesare Miglioli and Guillaume Blanc (teaching assistants) 2018-02-18 1 Introduction Hi cesare. ## Read this part first Anyone is invited to use any part of this eBook as long as credit is given. To cite this book, please use: Victoria-Feser, M.-P. (2018). A Lecture in Model Selection in High Dimensions, Research Center for Statistics, GSEM, University of Geneva, Switzerland. If you use this eBook as a reference for a course, please inform the author (maria-pia.victoriafeser@unige.ch). The content of this eBook is dynamic and changes as the lectures take place. Students participating to the classes can contribute to the content, with for example the analysis of real data sets, the resolution of exercises, simulations to explore methods in particular settings, etc. Their contribution is acknowledge where it is due. The first acknowledgements go to Cesare Miglioli and Guillaume Blanc, Ph. D. Students and the Research Center for Statistics, University of Geneva, for their invaluable contribution in setting up the first version of this eBook. 1.1 Content choice and structure The content of this e-book is intended for graduate and doctoral students in statistics and related fields interested in the statistical approach of model selection in high dimensions. Model selection in high dimensions is an active subject of research, ranging from machine learning and/or artificial intelligence algorithms, to statistical inference, and sometimes a mix of the two. We focus on the frequentist approach to model selection in view of presenting methods that have the necessary properties for out-of-sample (or population) validity, within an as large as possible theoretical framework that enables the measurement of different aspects of the validity concept. We therefore anchor the content into an inferential statistics approach, essentially for causal models. More specifically, the focus of model selection in high dimensions is presented into two main headings, one on statistical methods or criteria for measuring the statistical validity, and the other one on fast algorithms in high dimensional settings, both in the number of observation and in the number of inputs, that avoid the simultaneous comparison of all possible models. Even within this focus, the set of available methods is still very rich, so that only a selection of the available methods is presented. Each presentation is accompanied with practical exercises using R (https://www.r-project.org/). We highly recommend downloading RStudio’s IDE (https://www.rstudio.com/) which is an ideal working environment for statistical analyses. 1.1.1 Bibliography (to be completed) Computer Age Statistical Inference: Algorithms, Evidence, and Data Science. Bradley Efron &amp; Trevor Hastie, Cambridge University Press, 2016. https://web.stanford.edu/~hastie/CASI_files/PDF/casi.pdf An Introduction to Statistical Learning: with Applications in R. Gareth James, Daniela Witten, Trevor Hastie &amp; Robert Tibshirani, Springer, 2013. http://www-bcf.usc.edu/~gareth/ISL/ISLR%20First%20Printing.pdf The Elements of Statistical Learning: Data Mining, Inference, and Prediction. Trevor Hastie, Robert Tibshirani &amp; Jerome Friedman, Springer, 2009. https://web.stanford.edu/~hastie/ElemStatLearn/printings/ESLII_print12.pdf Model selection and model averaging. Gerda Claeskens and Nils Lid Hjort, Cambridge University Press, 2008. Regression and Time Series Model Selection. Allan D R McQuarrie and Chih-Ling Tsai, World Scientific, 1998. 1.1.2 Useful links (to be completed) Chamilo: https://chamilo.unige.ch/ (Search for Model Selection in High Dimensions to register) Github repository of the course: https://github.com/CaesarXVII/Model-Selection-in-High-Dimensions R project: https://www.r-project.org/ R Studio: https://www.rstudio.com/ An Introduction to Statistical Programming Methods with R: https://smac-group.github.io/ds/ GitHub: https://github.com/ UCI repository for datasets: https://archive.ics.uci.edu/ml/index.php Database: https://en.wikipedia.org/wiki/Database) Data Wrangling cheatsheet: https://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf Malnutrition in Zambia: https://archive-ouverte.unige.ch/unige:29628, p. 64 Data on Malnutrition in Zambia: Course Datasets American Cancer Association on Leukemia prognostic factors: https://www.cancer.org/cancer/leukemia-in-children/detection-diagnosis-staging/prognostic-factors.html Data on Leukemia in Children: http://web.stanford.edu/~hastie/CASI_files/DATA/leukemia_big.csv quantmod package in R: https://www.quantmod.com/ Theil inequality index: https://en.wikipedia.org/wiki/Theil_index Deep Learning: http://www.deeplearningbook.org/ Alzeihmer data: https://tadpole.grand-challenge.org/ 1.2 Using R There are many available classes, textbooks, e-books, etc. on how to get acquainted with a quite sophisticated usage of the most commonly used statistical software R (https://www.r-project.org/). The choice of the R editor depends on how R is used and for this class, we propose the open source editor RStudio (https://www.rstudio.com/). We also highly recommend the introduction proposed in https://smac-group.github.io/ds/index.html#r-and-rstudio, which will constitute the basic knowledge from which this class starts. 1.2.1 Useful R packages The R packages that will be used throughout this class are the following (to be completed): rmarkdown quantmod plotly tydir dplyr RODBC pool RMySQL foreign leaps glmulti MuMIn caret glmnet To install a package, use the R command install.packages(&quot;chosen.package.name&quot;) (see also https://cran.r-project.org/web/packages/). Visit the CRAN R project (https://cran.r-project.org/) to get useful information about all the available R packages (https://cran.r-project.org/web/packages/available_packages_by_name.html) 1.2.2 Managing Data (by G. Blanc) Data are nowadays continuously produced and readily available from internet platforms. This has become necessary since very often personal computer memory is not sufficient to store (high dimensional) data locally. It is therefore important to be able to import data into R for data analysis in an (almost) automatic fashion. Colloquially, `loading’ a dataset means storing it into your computer’s Random-Access-Memory (RAM), which allows for a fast access of the CPU to the data. The RAM is extremely fast1, but is typically of limited amount compared to what can be stored in a hard-drive or a SSD (usually 4 to 16 Gbs in a typical consumer-grade computer). The data stored in your computer’s RAM is called : the information it contains will disappear once the computer is shut down. This is the reason why the datasets must be loaded into R at the beginning of each session. In this section, you will learn different ways to load the data that you may encounter in the future, depending on the context and the size of the databases: from an R package, from a local data file, from an online data file, and from an online database. In this latter case, datasets are obtained by way of SQL queries via a remote connection. Follow along and load all the datasets as in the text: they will be used for the following exercises. 1.2.3 Loading data from an R package (by G. Blanc) Some packages have their own data included, and R-base indeed includes a well known selection, for instance the iris dataset. You can load it using: # Load the iris dataset iris &lt;- iris and display its structure with: # Display iris&#39; structure str(iris) ## &#39;data.frame&#39;: 150 obs. of 5 variables: ## $ Sepal.Length: num 5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ... ## $ Sepal.Width : num 3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ... ## $ Petal.Length: num 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ... ## $ Petal.Width : num 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ... ## $ Species : Factor w/ 3 levels &quot;setosa&quot;,&quot;versicolor&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... 1.2.4 Loading data from a local file (by G. Blanc) Typical file formats include .txt and .csv, or .data. Download the file wine.data from http://archive.ics.uci.edu/ml/machine-learning-databases/wine/ to a folder on your computer, say in ./datasets/. Every line of the file becomes a row in the dataset. You can load the data using the read.table function, which takes as further input sep = &quot;,&quot; to indicate that the variables in each line are separated by a comma. These variables will be organized as columns in the dataset. wine &lt;- read.table(&quot;./datasets/wine.data&quot;, sep = &quot;,&quot;) 1.2.5 Loading data from an online file (by G. Blanc) The end result will be the same as above, but is less tedious, provided you have an internet connection. Simply load the data using the complete url as an input for read.table: wine &lt;- read.table(&quot;http://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data&quot;, sep = &quot;,&quot;) # format the data.frame: colnames(wine) &lt;- c(&quot;Class&quot;, &quot;Alcohol&quot;, &quot;Malic_acid&quot;, &quot;Ash&quot;, &quot;Alcalinity_of_ash&quot;, &quot;Magnesium&quot;, &quot;Total_phenols&quot;, &quot;Flavanoids&quot;, &quot;Nonflavanoid_phenols&quot;, &quot;Proanthocynins&quot;, &quot;Color_intensity&quot;, &quot;Hue&quot;, &quot;OD280vsOD315&quot;, &quot;Proline&quot;) wine$Class &lt;- as.factor(wine$Class) 1.2.6 Loading data from an online database using a mySQL query (Optional) (by G. Blanc) The most common way to store massive, related data from different sources is to use relational databases. These consists of multiple datasets (tables), that may be related in some specific way. For instance, an online shop may have a table listing all of its registered clients, and another table listing all the orders made by the clients. In this example, the clients will have a unique identifier that will establish the relation between the two databases. A database is an organized collection of data. A relational database (see also https://en.wikipedia.org/wiki/Database), more restrictively, is a collection of schema, tables, queries, reports, views, and other elements. A database-management system (DBMS) is a computer-software application that interacts with end-users (you), other applications that you may develop (e.g., an Rmarkdown document), and the database itself to capture and analyze data. Relational databases organize data into one or more tables of columns and rows, with a unique key identifying each row. Generally, each table/relation represents one “entity type” (such as customer or product). The rows represent instances of that type of entity (such as “Lee” or “chair”) and the columns represent values attributed to that instance (such as “address” or “price”). The databases can be stored - offline, on a non-volatile memory (for instance your hard drive or SSD drive), or - online, which require an internet connection to access. You will usually need credentials (a username and a) to access either databases. As we will see, there are many advantages of relational databases which explain their almost universal use when it comes to storing massive amount of data online. The main advantages are that they: avoid data duplication, avoid inconsistent records, allow easily to change or add/remove the data, are more secure. To access the data store on the database, you will need to establish a connection, for which we will need RStudio’s pool package. We can explore the database and list the tables that it contains using dbListTables: # Explore the tables available in the database: dbListTables(con) ## [1] &quot;City&quot; &quot;Country&quot; &quot;CountryLanguage&quot; Remember that each table is a different dataset. Some databases are very big, in the BigData sense: think millions, even billions of entries. SQL queries allow to cherry pick the data we need, without having to download the whole dataset (which would be in some cases unfeasible). In a typical use, we would then query the data that we need, and no more. Since this is not a mySQL course and our datasets are of reasonable size, we will simply download the three datasets entirely using the SQLquery SELECT * FROM (Table). # Download the three datasets City &lt;- DBI::dbGetQuery(con, &quot;select * from City&quot;) Country &lt;- DBI::dbGetQuery(con, &quot;select * from Country&quot;) CountryLanguage &lt;- DBI::dbGetQuery(con, &quot;select * from CountryLanguage&quot;) poolClose(con) Exercise: - Load the iris dataset from the R-base package - Load the wine dataset from a local file and from an online file using the URL directly - Load the datasets City, Country, and CountryLanguage by connecting to an online database 1.2.7 Data Wrangling (by G. Blanc) Data wrangling means to manipulate and prepare a dataset in such a way, that it becomes amenable to analysis. Minimally, a numerical dataset is stored as a Matrix object, a type optimized for computations. Preferably, however, a dataset is stored as a dataframe object. A dataframe is technically a list of columns, each containing data of a given type (e.g., integer, numerical, character, factor). Many packages, and indeed R-base itself, are optimized to have the data organized in the following way: each row represent one observation each column represents a variable (or `feature’) There are two packages dedicated to data wrangling in R: dplyr is a grammar of data wrangling, which focuses on efficient and elegant coding data.table is computationally extremely fast to manipulate very large datasets Both create objects that are extensions of dataframes: dplyr uses a tibble, and data.table uses a data.table. We do not recommend to use data.table in this course, as it has a steeper learning curve. Using dplyr is not necessary either to complete the course; however, it will save you time in the end and is a worthwhile investment. For a quick reference, download the official cheatsheet. Exercise: We will assume that the datasets iris, wine, City, Country, and CountryLanguage of the above section have all been loaded in R, as well as the package dplyr. library(dplyr) Comment on the type of variables of the iris dataset. Write a code that computes the mean of each variable, grouped by Species. Do it first using R-base only, and optionally, do it using dplyr. variables.to.average &lt;- c(&quot;Sepal.Length&quot;, &quot;Sepal.Width&quot;, &quot;Petal.Length&quot;, &quot;Petal.Width&quot;) species.types &lt;- c(&quot;setosa&quot;, &quot;versicolor&quot;, &quot;virginica&quot;) # levels(iris$Species) # # Method 1A: without dplyr using a for loop; outputs a matrix species.means1 &lt;- NULL for(type in species.types){ is.species &lt;- iris$Species==type species.subset &lt;- iris[is.species, variables.to.average] species.means1 &lt;- rbind(species.means1, colMeans(species.subset)) } rownames(species.means1) &lt;- species.types # # Method 1B: more compact and efficient; outputs a matrix species.means2 &lt;- t(sapply(species.types, function(species){ colMeans(iris[iris$Species==species, variables.to.average]) })) # # Method 2: using dplyr; outputs a dataframe species.means3 &lt;- iris %&gt;% group_by(Species) %&gt;% summarise_all(mean) species.means3 ## # A tibble: 3 x 5 ## Species Sepal.Length Sepal.Width Petal.Length Petal.Width ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 setosa 5.01 3.43 1.46 0.246 ## 2 versicolor 5.94 2.77 4.26 1.33 ## 3 virginica 6.59 2.97 5.55 2.03 Using the method of your choice, plot the boxplots of alcohol levels of the three classes of wine from the wine dataset. boxplot(Alcohol ~ Class, data=wine, xlab=&quot;Class of wine&quot;, ylab=&quot;Alcohol concentration&quot;) Create 6 boxplots displaying the alcohol level of wine by class (3 levels) and by degree of color intensity (2 levels): above or below 5.0. (optional). Compute the corresponding means using dplyr. # Create a new binary variable, called Color_intensity_factor to denote whether the color is mild or intense. wine$Color_intensity_factor &lt;- factor(wine$Color_intensity&gt;5, levels=c(FALSE, TRUE), labels=c(&quot;mild&quot;, &quot;intense&quot;)) # Create the boxplots using the standard formulae boxplot(Alcohol ~ Color_intensity_factor + Class, data=wine, xlab=&quot;Class of wine&quot;, ylab=&quot;Alcohol concentration&quot;) # # Compute the alcohol means of each subgroup using dplyr wine %&gt;% group_by(Class, Color_intensity_factor) %&gt;% summarise_at(&quot;Alcohol&quot;, mean) ## # A tibble: 6 x 3 ## # Groups: Class [?] ## Class Color_intensity_factor Alcohol ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 1 mild 13.4 ## 2 1 intense 13.9 ## 3 2 mild 12.3 ## 4 2 intense 12.4 ## 5 3 mild 13.0 ## 6 3 intense 13.2 Consider the three datasets City, Country, and CountryLanguage. Merge the three datasets into a single one using the method of your choice, such that no information is lost. Compare the total number of entries in the three relational databases, to that of the unique database. What do you notice? [Hint: notice that “country” is the common relation among the three datasets] # We observe that &quot;country&quot; is the common relation among all datasets. # Let&#39;s try to merge all datasets into a single one using this key # # Merge all cities with country attributes data.merged &lt;- left_join(City, Country, by=c(&quot;CountryCode&quot; = &quot;Code&quot;)) %&gt;% left_join(CountryLanguage, by=&quot;CountryCode&quot;) # we observe that all the data pertaining to the countries appear multiple times. #The total number of data &quot;cells&quot; of the merged data set is 27031*22 = 594682, as compared to the original 24656 data &quot;cells&quot;, that is about 24 times bigger to store the same information. 1.3 Writing reports In this Section, information is provided about one convenient way to produce reports when working in teams. To be able to participate in the construction of this eBook, only text (including R chunks) in RMarkdown will be accepted. 1.3.1 R Markdown RMarkdown is a framework that provides a literate programming format for data science. It can be used to save and execute R code within R Studio and also as a simple formatting syntax for authoring HTML, PDF, ODT, RTF, and MS Word documents as well as seamless transitions between available formats. For example this eBook is written using R Markdown. We recommend the introduction proposed in https://smac-group.github.io/ds/rmarkdown.html to rapidly get acquainted with the use of R Markdown. Exercises with Iris dataset (see Loading data from an R package) Create an .rmd file from R Studio classic interface and look at the basic notions explained in the new document Create an histogram of the sepal width of Iris Setosa without showing both the code and the graph. Then, in another code chunk, show only the graph (without the code) and change the height or width of the histogram as you prefer Write the formula, both inline and with a Latex environment (e.g. equation), of the conditional probability of observing an Iris Virginica given that the sepal width is greater than 3. Display both the code and the conditional probability sub_joint = subset(iris,iris$Species == &quot;virginica&quot; &amp; iris$Sepal.Width &gt; 3) sub_marg = iris$Sepal.Width[iris$Sepal.Width &gt; 3] result = dim(sub_joint)[1]/length(sub_marg) result ## [1] 0.2537313 1.3.2 GitHub GitHub (https://github.com/) is a development platform designed to host and review code, manage projects, and build software alongside millions of other developers. An introduction to the use of Github for managing projects (e.g. a data analysis project), we recommend https://smac-group.github.io/ds/github.html. Students following this course will be stimulated to complete the exercises and practicals and provide their solutions that will be published in the eBook. GitHub provides a platform for team work that is strongly encouraged. Exercises Create a free GitHub account on https://github.com/ Read chapter 3 of the GitHub Guide https://smac-group.github.io/ds/github.html Install a version of Git (from https://git-scm.com/downloads) which is compatible with the OS of your computer (e.g. Windows/Mac/Linux/Solaris). Once you have downloaded and installed Git, the first thing you should do is to configure it by setting your username and email address (see first point). Watch the video in Section 3.3 of the GitHub Guide on the workflow within R Studio Create a new R Studio project, following the steps highlighted in the video, and take the URL from the GitHub repository of the course https://github.com/CaesarXVII/Model-Selection-in-High-Dimensions Modify the file (add the name of practical1.rmd file) as you like (e.g. try to solve an exercise). Then commit the changes and push it to the remote repository of the course. Do not forget to click on pull every time you access to your R Studio project to retrieve the updated version of all the files of the course repository In order to properly execute your commits, you need to be added as a collaborator of the project. It is sufficient to send an email to cesare.miglioli@etu.unige.ch with your GitHub name in it from your unige mail account and you will be set as a collaborator. 1.4 Examples 1.4.1 Data on Malnutrition in Zambia Childhood malnutrition is considered to be one of the worst health problems in developing countries (United Nations Children’s Fund 1998). Both a manifestation and a cause of poverty, malnutrition is thought to contribute to over a third of death in children under five years old globally (United Nations Children’s Fund 2012). Moreover, it is well established in the medical literature that maternal and child under nutrition have considerable consequences for adult health and human capital (see e.g. Victora et al. (2008) and the references therein). Such conditions are, for example, associated with less schooling, reduced economic productivity, and for women lower offspring birth weight. It has also been reported that lower birth weight and under nutrition in childhood have an influence on cancer occurrence and are risk factors for high glucose concentrations, blood pressure, and harmful lipid profiles. See also https://archive-ouverte.unige.ch/unige:29628, p. 64. Under nutrition is generally assessed by comparing anthropometric indicators such as height or weight at a certain age to a reference population. A well established measurement for the study of acute malnutrition is given by (see cite {who1995physical} for details): \\[\\begin{equation} Y_i = \\frac{H_{i,j} - \\mu_j}{\\sigma_{j}} \\label{eq:Zscore} \\end{equation}\\] where \\(H_{i,j}\\), \\(\\mu_j\\) and \\(\\sigma_j\\) denote, respectively, the height of the \\(i^{\\text{th}}\\) child at age \\(j\\), the median height of a child of the same age in the reference population and the associated standard deviation. Several factors are assumed to have a determinant influence on under nutrition. Consider the dataset Zambia.SAV available at Course Datasets containing variables assumed to be potential causes for childhood malnutrition, i.e. breastfeeding duration (month); age of the child (month); age of the mother (years); Body Mass Index (BMI) of the mother (kg/meter\\(^2\\)); height of the mother (meter); weight of the mother (kg); region of residence (9 levels: Central, Copperbelt, Eastern, Luapula, Lusaka, Northern, Northwestern, Southern and Western); mother’s highest education level attended (4 levels: No education, Primary, Secondary and Higher); wealth index factor score; weight of child at birth (kg) ; sex of the child; interval between the current birth and the previous birth (month); and main source of drinking water (8 levels: Piped into dwelling, Piped to yard/plot, Public tap/standpipe, Protected well, Unprotected well, River/dam/lake/ponds/stream/canal/ irrigation channel, Bottled water, Other). Exercise: Load the dataset and build the variables so that they can be used for a regression analysis. Associate proper names to each variable (hint: look at the comments in the r chunk). Perform a linear regression on all the available variables. Reduce the number of covariates (e.g. using the t-test) and add some interactions. Perform a linear regression on the new dataset. Analyse your chosen estimated model with a residual analysis (e.g. residuals vs fitted plot, normal QQ plot etc.). 1.4.2 Prognostic Factors in Childhood Leukemia (by C. Miglioli) Factors that can affect a child’s outlook (prognosis) suffering e.g. from Leukemia are called prognostic factors. They help doctors decide whether a child with leukemia should receive standard treatment or more intensive treatment. Prognostic factors seem to be more determinant in acute lymphocytic leukemia (ALL) than in acute myelogenous leukemia (AML). See https://www.cancer.org/cancer/leukemia-in-children/detection-diagnosis-staging/prognostic-factors.html for a detailed explanation. The leukemia_big.csv dataset contains gene expression measurements on 72 leukemia patients: 47 ALL (i.e. acute lymphocytic leukemia) and 25 AML (i.e. acute myelogenous leukemia). These data arise from the landmark of Golub et al. (1999) Science paper and exhibit an important statistical challenge because \\(p &gt;&gt; n\\) as we deal with 72 patients and 7128 measurements. Exercises Load the data from the URL http://web.stanford.edu/~hastie/CASI_files/DATA/leukemia_big.csv Create the response variable y according to the number of ALL and AML patients. In the same fashion create the matrix X of independent variables. See https://web.stanford.edu/~hastie/CASI_files/DATA/leukemia.html for further details. Fit a GLM (choose the correct link) to estimate the relationships between the outcome and the factors with the leukemia_big.csv dataset. Comment on the results that you obtain. 1.4.3 R package quantmod The quantmod package for R (https://www.quantmod.com/) is designed to assist the quantitative trader in the development, testing, and deployment of statistically based trading models. We are here interested in the easy and rapid access to data. It is possible with one quantmod function to load data from a variety of sources, including Yahoo! Finance (OHLC data), Federal Reserve Bank of St. Louis FRED® (11,000 economic series), Google Finance (OHLC data), Oanda, The Currency Site (FX and Metals), etc. Below are some examples on how to load financial data and perform some simple data analysis. A getting started guide to quantmod can be found at https://www.quantmod.com/examples/intro/. The first step is to install the quantmod package (only once), using the install.packages in R. Also install the plotly package for nice plots. Then try the following: # Load quantmod # library(quantmod) # Download data # today &lt;- Sys.Date() # three_month_ago &lt;- seq(today, length = 2, by = &quot;-3 month&quot;)[2] # getSymbols(&quot;AAPL&quot;, from = three_month_ago, to = today) # getSymbols(&quot;NFLX&quot;, from = three_month_ago, to = today) # Produce a # candleChart(NFLX, theme=&#39;white&#39;) # candleChart(AAPL, theme=&#39;white&#39;) 1.5 Fundamental statistical concepts 1.5.1 Sample and population If data are collected, stored, analysed, it is because the are supposed to provide information that cannot be otherwise available. Most of the information that is sought concerns sufficiently general phenomena that, in fact, nobody know (or will ever know) exactly. As an illustration, take the example of a teacher that computes the average score of the last math test in his class, what he gets is the exact information about the average score for that particular class and particular test, at the particular moment when the test took place and when the teacher marked the copies. Any other inference from the available information (the sample) to another context is subject to sampling variability and hence is not exact. If the teacher uses the average score to somehow evaluate the difficulty of his math test, then what he has observed within his class is only a part of the truth. For that purpose (evaluating the difficulty of the math test), he should let all the possible students (the population) pass the test and compute the average of the resulting scores of all of them. This is of course not possible, but statistical methodology can help in targetting the question of interest summarized here by the scores average, by providing, for example, a finite set of possible values for the true average, the one computed virtually on the population, also called a parameter. 1.5.2 Models and risk When the sample per se is not the target (i.e. in most of the cases), then one enters into the process of inference: what can we say about what happens in the population, given a sample of data, supposedly carrying enough information for that purpose? A fundamental aspect of statistical inference is the ability of constructing (manageable) measures of variability to any data treatment operated in order to produce information that is used, in its context, to e.g.: understand the phenomenon under investigation, to predict, to evaluate research hypotheses, etc. The inference concept implies two subsequent questions that are at the core of statistics. On the one hand, one has to define what is the population information of interest, and, on the second hand, one has to provide an inferential risk, i.e. a measure of risk associated to any inference made from the sample to the population. The first concept can be associated, very broadly, to the model, i.e. a set of input (a priori) information that serves to formalize the information of interest. The second concept which is a direct consequence of a function of the sampling variability, can be associated to a (set of) propability, a fundamental measure in statistics. Sometimes, and even more and more often, the two concepts are untangled, in the sense that the model can be very flexible (it is actually a set of models) and a risk measure is used to somehow define a (or a drastically reduced set of) model. This vas-et-vient process could be used to define model selection in statistics. Finally, while the model (or the set of models) is, in general, set a priori, the inferential risk needs to be estimated from the available information, i.e. the sample itself. For that purpose, the fundamental instrument is probability theory. 1.5.3 Estimators and associated variability Consider the simplest decisional setting, i.e. confidence intervals for population parameters. We adopt here a frequentist approach. Population parameters can be quantities of interest, e.g. the population mean, the population proportion (for something specific), the population probability (e.g. of being bankrupt or of surviving a given treatment), or more elaborate quantities such as inequality or poverty measures (see e.g. Cowell (2011)). Very generally, consider an estimator \\(\\hat{\\theta}\\) from a population parameter \\(\\theta\\in\\Theta\\subseteq\\mathbb{R}^p\\) that is computed on a sample \\(F^{(n)}\\)2 generated for a (family of) model \\(F\\). The latter can be parametric, non parametric or semi-parametric. We can write \\(\\hat{\\theta}(F)\\), i.e. the estimator as a functional (or function of a distribution) of \\(F\\); in particular, we can write \\(\\hat{\\theta}(F^{(n)})\\). For example, the sample mean, an estimator of the population mean \\(\\mu\\), can be written as: \\[\\begin{equation} \\hat{\\mu}\\left(F^{(n)}\\right)=\\int x dF^{(n)}(x)=\\frac{1}{n}\\sum_{i=1}^n x_i \\end{equation}\\] An estimator is first chosen for the population parameter and then a confidence interval is built (estimated from the data) that depends on some underlying assumptions about the data generating process. This requires calculating the properties of estimators \\(\\hat{\\theta}(F^{(n)})\\) at distribution \\(F\\) (which is unknown, only assumed). To do so, there are several strategies which include (see Efron and Hastie (2016), chapter 2): The plug-in principle: The variance (or any other moment of the distribution) of \\(\\hat{\\theta}(F^{(n)})\\) is expressed (theoretically) as a function of population parameters (e.g. the population mean, variance, higher moments) and the population parameters in the formula are replaced by estimators computed from the sample. General results on classes of estimators such as the maximum likelihood estimator (MLE) or \\(M\\)-estimators (Huber and Ronchetti 2009) can be used for the plug-in principle. Taylor-series approximations: Let \\(T(\\hat{\\theta})\\) be a function of interest of \\(\\hat{\\theta}(F^{(n)})\\), one can use local linear approximations, method that is also sometimes known as the delta method. One considers the linear expansion of \\(T(\\hat{\\theta})\\) around \\(T(\\theta)\\) and uses the approximation \\(T(\\hat{\\theta})=T(\\theta)+\\partial/\\partial\\theta^T T(\\theta)\\left(\\hat{\\theta}-\\theta\\right)\\) together with the plug-in principle to get an estimator of \\(T(\\hat{\\theta})\\). For example, the variance of \\(T(\\hat{\\theta})\\) can be estimated by \\(\\left(\\partial/\\partial\\theta^T T(\\theta)\\right)\\text{var}(\\hat{\\theta})\\left(\\partial/\\partial\\theta^T T(\\theta)\\right)^T\\) in which the unknown \\(\\theta\\) is replaced by its estimated value. Using the functional notation, approximations can be found using von Mises expansions, together with Gâteaux differentials for multidimensional functionals (see e.g. Fernholz (2001)). Simulation and the bootstrap: The basic idea is to implement the infinite sequence of future trials using simulated samples in almost infinite quantities. An estimator \\(\\hat{F}\\) of \\(F\\) is first chosen, then samples \\(F_k^{(n)},k=1,\\ldots,B\\) (\\(B\\) is the almost infinite quantity) are simulated from \\(\\hat{F}\\) to compute estimates \\(\\hat{\\theta}^{(k)}:=\\hat{\\theta}(F_k^{(n)})\\). This produces an estimate for the distribution of \\(\\hat{\\theta}\\) that can be used to compute the required quantities such mean, variance, quantiles, etc. Simulation based inference is quite different to traditional methods based on the plug-in principle in that an estimator is sought for \\(F\\) instead of estimators for population parameters. The natural question to ask at this point is what is the best approach to measure risk (sampling error) associated to a statistic (a functional of the sample distribution)? In a frequentist paradigm, one can rely on concepts such as the minimum variance or mean squared error of the resulting estimator (an asymptotic concept also called efficiency) and/or the rate of convergence of the resulting estimator (related to the asymptotic concept of consistency), i.e. how does the estimator converge to the corresponding population quantity as a function of the sample size \\(n\\). There is no unifying theory providing an optimality result for all settings, rather general results for classes of models, estimators and/or simulation-based methods. 1.5.4 Simulating the population using resampling techniques (See Efron and Hastie (2016), chapter 10) Generally speaking, resampling techniques allow to simulate the population, or more precisely, the sampling mechanism of an infinte number of trials (samples). The unspoiled and unique proxy for the population (model) is the sample \\(F^{(n)}\\). For finite populations of size \\(N\\) (that can be huge), a sample of independently drawn observations (i.e. the iid case) can be seen as one realization of \\(n\\) draws from a multinomial distribution with \\(N\\) equally probable (\\(1/N\\)) outcomes corresponding to the \\(N\\) population values. This suggests that a suitable proxy to this data generating mechanism (in the iid case) is to proceed with \\(n\\) draws from a multinomial distribution with \\(n\\) equally probable (\\(1/n\\)) outcomes corresponding to the \\(n\\) sample values. This is what the non parametric Bootstrap (Efron 1979) does. Exercise (optional): The aim here is to reproduce the non parametric Bootstrap and compare it with Monte Carlo simulations. Use as an example the Theil Inequality Index (see e.g. https://en.wikipedia.org/wiki/Theil_index) together with the Generalized Beta Distribution of the second kind. For the sampling part, control the seed (see help(set.seed) in R). Use both the sample command in R and the Uniform distribution to produce the random draws. - Produce (with both methods) the Bootstrap estimate of the sampling distribution of the Theil Inequality Index (of size 1000) for one chosen set of values for the parameters of the Generalized Beta Distribution of the second kind. Check that both methods produce exactly the same distribution’s estimate. - Do the same using a Monte Carlo experiment and compare the outcome with the Bootstrap estimate (control the seed). The jackknife (Quenouille (1956), Tukey (1958)) was a first step toward simulation-based inference, developed to compute standard errors. It actually produces \\(n\\) systematic (not randomly drawn) samples \\(F_{(i)}^{(n)},i=1,\\ldots,n\\), each one of size \\(n-1\\), obtained by successively removing one observation at the time. For the functional \\(\\hat{\\theta}(F^{(n)})\\), the (unbiased) Jackknife standard error (SE), say \\(T\\), is given by (see Efron and Stein (1981)) \\[\\begin{equation} T(\\hat{\\theta})=\\left[\\frac{n-1}{n}\\sum_{i=1}^n\\left(\\hat{\\theta}\\left(F_{(i)}^{(n)}\\right)-\\bar{\\hat{\\theta}}\\right)\\right]^{1/2} \\end{equation}\\] with \\(\\bar{\\hat{\\theta}}=1/n\\sum_{i=1}^n\\hat{\\theta}\\left(F_{(i)}^{(n)}\\right)\\). The jackknife can be seen as a linear approximation of the bootstrap, hence less appropriate for unsmooth estimators (e.g. quantiles). Exercise (optional) - Use the sample experimental setting as in the exercise for the Boostrap and compute the sampling distribution estimate of of the Theil Inequality Index using the jacknife. Compare the results with the ones based on the Bootsrap and Monte Carlo simulations. An alternative and natural estimator for \\(F\\) is to assume that \\(F\\) belongs to a family of (parametric) distributions (models), indexed by a parameter vector \\(\\theta\\), i.e. the set \\(\\{F_{\\theta}, \\theta\\in\\Theta\\subseteq\\mathbb{R}^p \\}\\) and, using the plug-in principle, one gets \\(\\hat{F}=F_{\\hat{\\theta}}\\). This requires some attention for the choice of \\(\\hat{\\theta}\\) which ad minima should be consistent. The MLE (computed on the original sample \\(F^{(n)}\\)) is a suitable candidate for consistency, but also for efficiency. To produce \\(B\\) samples of size \\(n\\), there exists various (implemented) random generators, which are in principle based on a random generation of Uniform(0,1) realizations \\(u_i,\\ldots,u_n\\), from which a sample is obtained via \\(x_i=F_{\\hat{\\theta}}^{-1}(u_i), i=1,\\ldots,n\\). Parametric families act as regularizers, smoothing out the raw data and de-emphasizing extreme observations. They are particularly appreciated when studying rare events, like probabilities of extremes, in finance, insurance and with natural phenomena (tides, temperatures, earthquakes, etc.). The obvious drawback is that they need to be specified a priori, but the family can be sufficiently large. In this case, model selection becomes an important step into model building, with an obvious impact on inference. Exercise (optional): There are different ways to simulate samples in the linear regression model case. In the non parametric case, the design matrix is kept fixed or, alternatively, the raws are drawn together with the response. It is not yet clear which method is the most suitable in therms of statistical properties of resulting procedures like significance testing. In the parametric case, only the random part is simulated, i.e. the residuals (with \\(\\hat{\\beta}\\) and \\(\\hat{\\sigma}\\)). There is also a semi-parametric version which consists in resampling the residuals. - With the Malnutrition in Zambia dataset, considering the (complete) linear model without interactions, compute 95% confidence intervals (percentile method) for the slope parameter of the breastfeeding duration variable, using the four different resampling schemes for the linear regression model. Compare. Hint: Read chapter 11 of Efron and Hastie (2016). 1.5.5 Model Selection Model selection is a broad concept. For example, for a family of models \\(F_{\\mathbf{\\theta}}\\), there exists an (almost) infinite number of different ones according to the value of \\(\\mathbf{\\theta}\\). Hence, estimation (parametric or non parametric) is also a form of model selection since it allows, from the sample, to reduce the set of potential models. Another form of model selection involves, simultaneously, the specification of the parameter’s set \\(\\mathbf{\\theta}\\) (e.g. what is \\(p\\)) and, within this specification, a reduced set of potential values. This process, like estimation, is by nature inferential, since the only available information is the sample. A trade-off needs then to be made between the model complexity (e.g. \\(p\\)) and the model edequacy (e.g. its fit to the data). The choice of the measure associated to the trade-off is also important. While model adequacy is an obvious objective to achieve, reducing model complexity is a more subtle, but also an important feature. The reasons include (see also James et al. (2013), Section 2.1.3): When we are mainly interested in inference, then restrictive models are much more interpretable. When the objective is prediction only and the interpretability of the predictive model is simply not of interest, we might expect that it will be best to use the most flexible model available. Surprisingly, this is not always the case, since more accurate predictions are obtained using a less complex models (see Section 2.1) One can separate model selection procedures in three broad categories which are: - Subset Selection: This approach involves identifying a subset of the \\(p\\) predictors (i.e. a non zero subset of \\(\\mathbf{\\theta}\\)) that we believe to be related to the response. - Shrinkage: This approach involves fitting a model involving all \\(p\\) predictors (or parameters in \\(\\mathbf{\\theta}\\)). However, the estimated parameters are shrunken towards zero relative in the estimation procedure. This shrinkage is also known as regularization and has the effect of reducing sampling error and, depending on what type of shrinkage is performed, some of the coefficients may be estimated to be exactly zero, leading to a form of subset selection. - Dimension Reduction: This approach involves reducing the \\(p\\) predictors into a \\(q\\)-dimensional subspace, where \\(q &lt;p\\), using a trade-off between information loss due to the dimension reduction and model complexity. The \\(q\\) (orthogonal) axes of the subspace are then used as predictors to fit the model. Obviously, subset selection and shrinkage are methods that target the model interpretability objective, while dimension reduction might be more appropriate for pure prediction. It is however not clear that, in terms of out-of-sample prediction error, one set of approaches is better than the others. In this course we will mainly focus on subset selection while also presenting shrinkage methods. References "],
["assessing-the-validity-of-a-model.html", "2 Assessing the validity of a model 2.1 Introduction 2.2 Cross-Validation 2.3 Covariance Penalties Criteria 2.4 Information Theory and Bayesian Criteria 2.5 Mean Squared Error Based Criteria 2.6 Classification measures 2.7 Generalized measures", " 2 Assessing the validity of a model 2.1 Introduction Broadly speaking, model selection is about choosing a model that best fits (on some sense) the available sample so that it can be used to understand the phenomenon under investigation. This also includes being able to predict future outcomes and to assess research hypotheses within the population. Therefore, to reach these objectives, the available information should be used for two important tasks: building the model (learning phase) assess its out-of-sample validity The two tasks are interdependent since building the model, i.e. finding the most suitable one, is made by optimizing, in some sense, its out-of-sample validity. Out-of-sample validity is a crucial concept in model selection. Indeed, it allows to assess how the results of a statistical analysis (e.g. the selection of a model) will generalize to other outcomes, equivalently, to the population. Without out-of-sample validation, a selection procedure will necessarily tend to choose models that overfit the data, since they would be the best within the sample. In other words, when overfitting, a model describes random error or noise instead of the underlying relationship, leading to an excessively complex model (too many parameters relative to the number of observations) and consequently a poor predictive performance and a wrong representation of what happens in the population. Example of overfitting situation # Taken from https://en.wikipedia.org/wiki/Overfitting. # Noisy (roughly linear) data is fitted to a linear function and a polynomial function. # Although the polynomial function is a perfect fit, the linear function can be expected # to generalize better: if the two functions were used to extrapolate beyond the fit data, # the linear function would make better predictions. This is because the polynomial # function it is too dependent on the data which contains sampling error. # knitr::include_graphics(&quot;Figures/OverfitEx1.png&quot;) More formally, consider a random variable \\(Y\\) distributed according to model \\(F_{\\boldsymbol{\\theta}}\\), possibly conditionally on a set of fixed covariates \\(\\mathbf{X}=[x_1 \\ldots \\, x_p]\\). We observe a random sample \\(\\mathbf{y} = (Y_i)_{i = 1, \\ldots , n}\\) supposedly generated from \\(F_{\\boldsymbol{\\theta}}\\) together with a non-random \\(n\\times p\\) full rank matrix of predictors \\(\\mathbf{X}\\). Define the prediction function \\(\\hat{\\mathbf{y}}\\) that depends on the considered model, for example \\(\\hat{\\mathbf{y}}=\\hat{\\boldsymbol{\\beta}}\\mathbf{y}\\) for the linear regression model. The inferential task is to assess the accuracy of the prediction function, so that it can be compared to alternative prediction functions. Quantifying the prediction error of a prediction function requires specification of a discrepancy \\(D\\) between a prediction \\(\\hat{\\mathbf{y}}\\) and the actual response \\(\\mathbf{y}\\). A natural choice is the mean squared error (mean residual squared error) \\[\\begin{equation} D\\left(\\hat{\\mathbf{y}},\\mathbf{y}\\right)=\\frac{1}{n}\\sum_{i=1}^nd\\left(\\hat{y}_i,y_i\\right)=\\frac{1}{n}\\sum_{i=1}^n\\left(\\hat{y}_i-y_i\\right)^2 \\end{equation}\\] However, in order to avoid overfitting situations for out-of-sample validity, what is actually sought is the true prediction error, i.e. the (mathematically) expected discrepancy between prediction function out-of-sample \\(\\hat{y}_0\\) and the corresponding out-of-sample realization \\(y_0\\), namely \\[\\begin{equation} \\text{Err}=\\mathbb{E}_{0}\\left[d\\left(\\hat{Y}_0,Y_0\\right)\\right] \\end{equation}\\] where \\(\\mathbb{E}_{0}\\) denotes the expectation at the out-of-sample distribution. This quantity needs to be estimated in some manner with the sample at hand, hence the real challenge in model selction is - the specification of an out-of-sample validity measure, called a model selection criterion - estimators for model selection criteria - performance measures to compare different model selection criteria In the following sections we present several methods for model selection and measures for out-of-sample validity. Exercice (optional): - Consider the Malnutrition in Zambia dataset (without interections). As model fit assesment consider a) the residual variance and b) the R^2. Starting from the full model (without interactions), increase progressively model size by including polynomials (on the continuous variables) and observe the behavior of the model fit criteria (using e.g. a plot). Additional material: Controlling for out-of-sample validity follows the lines of the scientific approach, in particular Occam’s razor problem-solving principle (https://en.wikipedia.org/wiki/Occam's_razor). One can intuitively understand overfitting from the fact that information from all past experience can be divided into two groups: information that is relevant for the future (“signal”) and irrelevant information (“noise”). Everything else being equal, the more difficult a criterion is to predict (i.e., the higher its uncertainty), the more noise existing in past information needs to be ignored. The problem is to determine which part to ignore. See https://en.wikipedia.org/wiki/Overfitting. 2.2 Cross-Validation The challenge is now to find a suitable estimator for \\(\\text{Err}\\) with the help of the sample and the (assumed) model \\(F_{\\boldsymbol{\\theta}}\\). A first guess is the apparent error \\[\\begin{equation} \\text{err}= \\frac{1}{n}\\sum_{i=1}^nd\\left(\\hat{y}_i,y_i\\right) \\end{equation}\\] Unfortunatelly err usually underestimates Err since \\(\\hat{y}_i\\) has been adjusted to fit the observed responses \\(y_i\\) (trainig set). Ideally, one should have an independent validation set (or test set) of say \\(n^{\\star}\\) additional observations \\(y_{0i}, i=1,\\ldots,n^{\\star}\\), so that one could estimate Err using \\[\\begin{equation} \\widehat{\\text{Err}}_{\\text{val}}= \\frac{1}{n^{\\star}}\\sum_{i=1}^{n^{\\star}}d\\left(\\hat{y}_{0i},y_{0i}\\right) \\end{equation}\\] Cross-validation attempts to mimic \\(\\widehat{\\text{Err}}_{\\text{val}}\\) without the need for a validation set. Define \\(\\hat{y}_{(i)}\\) to be the predicted value compute on the reduced training set in which the \\(i\\)th observation has been omitted. The the leave one out cross-validation estimate of prediction error is \\[\\begin{equation} \\widehat{\\text{Err}}_{\\text{cv1}}= \\frac{1}{n}\\sum_{i=1}^{n}d\\left(\\hat{y}_{(i)},y_{i}\\right) \\end{equation}\\] A more common practice is to leave out several observations at a time: the sample is randomly partitioned into \\(J\\) groups of size about \\(\\lfloor n/J\\rfloor=n_J\\) each, then for each \\(j=1,\\ldots,J\\) groups, the training set is the sample without the \\(j\\)th group on which the prediction \\(\\hat{y}_{(j)}\\) is computed and then compared to the observations in the \\(j\\)th group \\(y_j\\) using the chosen discrepancy measure \\(D\\). A common choice is \\(J=10\\) leading to the ten-fold cross-validation procedure. Reducing from \\(n\\) to \\(J\\) the number of training and validation sets reduces the necessary number of prediction rules constructions (estimation). The optimal value for \\(J\\) is however not known. Exercise: - Program k-fold Cross-Validation (with k=2) and do model selection in a specific simulation setting with an exhaustive search. Follow these steps: (a) Generate from a MVN (multivariate normal) a matrix \\(\\mathbf{X_{n*p}}\\) with \\(n = 1000\\) and \\(p = 5\\). You can choose the location vector as you wish but set the scale matrix as the identity. (b) Choose the generating vector \\(\\boldsymbol{\\beta }= [3 \\; 1.5 \\; 0 \\; 2 \\; 0]\\) and retrieve the signal to noise ratio of this setting. (c) Generate \\(\\hat{\\mathbf{y}}\\) thanks to the relation \\(\\mathbf{y} = \\mathbf{X_{n*p}} \\; \\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}\\) where \\(\\epsilon_{i}\\) is a standard normal, \\(n = 1000\\) and \\(p = 5\\). Suppose for simplicity that the errors are uncorrelated. (d) Split the data randomly in two halves (k=2) and use the training set to determine \\(\\hat{\\boldsymbol{\\beta}}\\). Then, compute the squared loss function as prediction error measure for each possible model. Observe which model is the best model. (e) Suppose now that we increase the size of \\(\\boldsymbol{\\beta}\\) to 100 (i.e. \\(p = 100\\) ). Calculate the number of possible models to evaluate together with an estimate of the time needed for an exhaustive search (hint: use previous results). Conclude on the feasibility of this task. 2.3 Covariance Penalties Criteria 2.3.1 Introduction Originally, the covariance penalty approach treats prediction error estimation in a regression framework, with the predictors \\(\\mathbf{X}\\) considered as fixed. Moreover, supposing for the moment that the discrepancy measure is the squared difference (or \\(L_2\\) norm), the true prediction error (conditionnally on \\(\\mathbf{x}_i\\)) is defined as \\[\\begin{equation} \\text{Err}_i=\\mathbb{E}_{0}\\left[\\left(\\hat{Y}_i-Y_0\\right)^2\\right] \\end{equation}\\] The overall prediction error is \\(\\text{Err}=1/n\\sum_{i=1}^n\\text{Err}_i\\). The question is how to estimate this quantity, given a sample and a data generating model? (Efron 2004) uses \\(\\mathbb{E}\\left[\\text{Err}_i\\right]\\) (where \\(\\mathbb{E}\\) denotes the expectation at the insample distribution) and shows that \\[\\begin{equation} \\mathbb{E}\\left[\\text{Err}_i\\right]= \\mathbb{E}\\left[\\text{err}_i\\right]+2\\text{cov}\\left(\\hat{Y}_i;Y_i\\right) \\end{equation}\\] with \\(\\mathbb{E}\\left[\\text{err}_i\\right]= \\left(\\hat{y}_i-y_i\\right)^2\\), the apparent (or in-sample) error. This result says that, on average, the apparent error \\(\\text{err}_i\\) understimates the true prediction error \\(\\text{Err}_i\\) by the covariance penalty \\(2\\text{cov}\\left(\\hat{Y}_i;Y_i\\right)\\). This makes intuitive sense since \\(\\text{cov}\\left(\\hat{Y}_i;Y_i\\right)\\) measures the amount by which \\(y_i\\) influences its own prediction \\(\\hat{y}_i\\). An natural estimator for the overall predition error is then given by \\[\\begin{equation} \\widehat{\\text{Err}}= \\frac{1}{n}\\sum_{i=1}^n\\left(\\hat{y}_i-y_i\\right)^2+\\frac{2}{n}\\sum_{i=1}^n\\widehat{\\text{cov}}\\left(\\hat{Y}_i;Y_i\\right) \\end{equation}\\] Depending on the assumed model, \\(\\widehat{\\text{cov}}\\left(\\hat{Y}_i;Y_i\\right)\\) is obtained analytically up to a value of \\(\\boldsymbol{\\theta}\\), the model’s parameters, which is then replaced by \\(\\hat{\\boldsymbol{\\theta}}\\) (plug-in method), or by resampling methods such as the parametric bootstrap. For the later, considering the model \\(F_{\\boldsymbol{\\theta}}\\), one uses the following steps: Estimate \\(\\boldsymbol{\\theta}\\) from \\(F^{(n)}\\) to get \\(\\hat{\\boldsymbol{\\theta}}\\). Set the seed For \\(j=1,\\ldots,B\\), do Simulate \\(n\\) values \\(y_i^{(j)}, i=1,\\ldots,n\\) from \\(F_{\\hat{\\boldsymbol{\\theta}}}\\), Compute \\(\\hat{y}_i^{(j)}, i=1,\\ldots,n\\), possibly conditionally on the matrix of predictors \\(\\mathbf{X}\\) Compute \\(\\forall i\\) \\[\\begin{equation} \\widehat{\\text{cov}}\\left(\\hat{Y}_i;Y_i\\right)=1/B\\sum_{j=1}^B\\left(\\hat{y}_i^{(j)}-\\hat{y}_i^{(j\\cdot)}\\right)\\left(y_i^{(j)}-y_i^{(j\\cdot)}\\right) \\end{equation}\\] with \\(\\hat{y}_i^{(j\\cdot)}=1/n\\sum_{j=1}^B\\hat{y}_i^{(j)}\\) and \\(y_i^{(j\\cdot)}=1/n\\sum_{j=1}^By_i^{(j)}\\) Exercise (optional): Consider the simulation setting of the exercise in the previous Section. - Instead of splitting the sample in two halves, compute the covariance prenalized predition error, with covariance penalty estimated via simulations (using the proposed algorithm). - Compare the analysis (decison) with the one obtained by means of cross-validation (in the previous exercise). 2.3.2 Mallows \\(C_p\\) Consider the linear regression model \\(Y_i|\\mathbf{x}_i \\sim \\mathcal{N}(\\boldsymbol{\\mu}\\left(\\mathbf{x}_i\\right),\\sigma^2), 0&lt;\\sigma^2&lt;\\infty\\), with \\[\\begin{equation} \\boldsymbol{\\mu}\\left(\\mathbf{x}_i\\right)=\\mathbf{x}_i^T \\boldsymbol{\\beta}, \\end{equation}\\] where \\(\\boldsymbol{\\beta} \\in \\mathbb{R}^p\\) and \\(\\mathbf{x}_i^T\\) is the ith row of \\(\\mathbf{X}\\) (that includes a column of ones for the intercept). One notable result (see exercise below) that can be deduced from the covariance penalty formula, for the linear regression model using the least squares estimator (LSE) \\(\\hat{\\beta}=\\left(\\mathbf{X}^T\\mathbf{X}\\right)^{-1}\\mathbf{X}^T\\mathbf{y}\\), \\(\\mathbf{y}=[y_1,\\ldots,y_n]^T\\), is Mallow’s \\(C_p\\)3 (Mallows 1973): \\[\\begin{equation} C_p=\\frac{1}{n}\\sum_{i=1}^n\\left(\\hat{y}_i-y_i\\right)^2+\\frac{2}{n}p\\sigma^2 \\end{equation}\\] Exercise: derive Mallow’s \\(C_p\\) from the general estimator \\(\\widehat{\\text{Err}}\\). Exercise (optional): Consider a Linear Mixed Model (LMM), for example the electrode resistance data, estimated using the generalized least squares (GLS) estimator. - Derive and/or estimate \\(\\widehat{\\text{Err}}\\). Hint: write the model with a vector of stacked responses and, consequently, a non-diagonal residual error (co)variance. 2.3.3 Efron’s \\(q\\)-class Covariance penalties can be applied to measures of prediction error other than squared error, like the Kullback - Leibler divergence. Then, to derive \\(\\widehat{\\text{Err}}\\), one needs a more general expression for \\(\\mathbb{E}\\left[\\text{Err}_i\\right]\\). Efron (1986) uses a function \\(Q(\\cdot,\\cdot)\\) based on the \\(q\\)-class error measure between two scalar functions \\(u\\) and \\(v\\), given by \\[\\begin{equation} Q(u,v) = q(v) + \\dot{q}(v) (u - v) - q(u) \\end{equation}\\] where \\(\\dot{q}(v)\\) is the derivative of \\(q( \\cdot )\\) evaluated at \\(v\\). For example \\(q(v) = v(1-v)\\) gives the squared loss function \\(Q(u,v) = (u - v)^2\\) and \\(q(v)=\\min\\{v,(1-v)\\}\\) leads to the missclassification loss \\(Q(u,v)=I\\{u\\neq I(u&gt;1/2)\\}\\), where \\(I(\\cdot)\\) denotes the indicator function. Efron’s optimism Theorem (Efron 2004) demonstrates that \\[\\begin{equation*} \\text{Err}_i = \\mathbb{E} \\left[ \\mathbb{E}_0 \\left[ Q(Y^0_i,\\hat{Y}_i) | \\mathbf{y}\\right] \\right] =\\mathbb{E} \\left[ Q(Y_i,\\hat{Y}_i) + \\Omega_i \\right] \\label{eq:optimismTHM} \\end{equation*}\\] with \\(\\Omega_i = \\text{cov} \\left( \\dot{q}(\\hat{Y}_i),Y_i \\right)\\). Hence, an estimator of \\(\\text{Err}\\) is obtained as \\[\\begin{equation} \\widehat{\\text{Err}} = \\frac{1}{n} \\sum_{i = 1}^{n}\\left( Q(y_i,\\hat{y}_i) + \\widehat{\\text{cov}} \\left( \\dot{q}(\\hat{Y}_i),Y_i \\right)\\right) \\end{equation}\\] 2.4 Information Theory and Bayesian Criteria 2.4.1 AIC: Akaike Information Criterion The AIC is derived from Information Theory which concerns the quantification, storage, and communication of information (Shannon (1948a),Shannon (1948b)). Associated measures are applied to distributions of random variables and include the entropy measure for a single random variable. A derived measure for two random variables is the Kullback-Leibler divergence (or information divergence, information gain, or relative entropy). Consider two densities \\(f_0\\) and \\(f_1\\), the Kullback–Leibler divergence is \\[\\begin{equation} D\\left(f_0,f_1\\right)=2\\int f_0(y) \\log\\left(\\frac{f_0(y)}{f_1(y)}\\right)dy = 2\\mathbb{E}_0\\left[\\log\\left(\\frac{f_0(y)}{f_1(y)}\\right)\\right] \\end{equation}\\] The Kullback-Leibler divergence can be used to evaluate the adequacy of a model, by considering e.g. \\(f_1:=f(y;\\hat{\\boldsymbol{\\theta}})\\), the fitted density corresponding to model \\(F_{\\boldsymbol{\\theta}}\\). In that case, the true prediction error becomes a deviance given by \\[\\begin{equation} \\text{Err}_i=2\\mathbb{E}_0\\left[\\log\\left(\\frac{f_0(y)}{f(y_i;\\hat{\\boldsymbol{\\theta}})}\\right)\\right] \\end{equation}\\] with total deviance \\(1/n\\sum_{i=1}^n \\text{Err}_i\\). Akaike (Akaike 1974) proposed to consider, as a model adequacy measure, an estimator of \\[\\begin{equation} 2\\mathbb{E}_0\\left[\\mathbb{E}\\left[\\log\\left(f_0(y)\\right)-\\log\\left(f(y;\\hat{\\boldsymbol{\\theta}})\\right)\\right]\\right] \\end{equation}\\] where \\(\\mathbb{E}\\) denotes the expectation at the insample distribution. Akaike derived the estimator \\[\\begin{equation} -2\\sum_{i=1}^n \\log f(\\hat{y}_i;\\hat{\\boldsymbol{\\theta}})+2p + \\text{const.} \\end{equation}\\] where \\(\\text{const.}\\) does not depend on the model and hence can be ommitted when comparing models. For the linear regression model with \\(\\hat{\\boldsymbol{\\mu}}=\\mathbf{x}^T \\hat{\\boldsymbol{\\beta}}\\), supposing \\(\\sigma^2\\) known (and ommitting the constant term), we have \\[\\begin{equation} \\frac{1}{\\sigma^2}\\sum_{i=1}^n \\left(y_i-\\mathbf{x}_i^T \\hat{\\boldsymbol{\\beta}}\\right)^2+2p \\end{equation}\\] There exist several expressions for the AIC for the linear regression model, one of them being \\[\\begin{equation} \\text{AIC}= \\frac{1}{n\\sigma^2} \\text{RSS}+\\frac{2}{n}p \\end{equation}\\] where \\(\\text{RSS}=\\sum_{i=1}^n\\left(y_i-\\mathbf{x}_i^T \\hat{\\boldsymbol{\\beta}}\\right)^2\\) is the residual sum-of-squares. We can see that \\(C_p=\\sigma^2\\text{AIC}\\) Exercise: - Derive the AIC for the regression model from its more general definition. - Program AIC and do model selection in a specific simulation setting with an exhaustive search (follow the passages listed in the CV exercise section). - Compare the performance of your programmed CV and AIC by replicating 100 times the tasks. In particular you should evaluate three specific criteria: the proportion of times the correct model is selected (Exact), the proportion of times the selected model contains the correct one (Correct) and the average number of selected regressors (Average \\(\\sharp\\)) - Load the wine dataset from the UCI repository and perform an exhaustive search based on CV and AIC in order to find the best model. You can either employ your codes derived in previous exercises or make use of the existing R packages: leaps, glmulti, MuMIn and caret. Exercise (optional): Using the general result on covariance penalty measures based on Efron’s \\(q\\)-class, show that the AIC is a suitable estimator of the prediction error. 2.4.2 BIC: Bayesian Information Criterion (Schwarz 1978) derived the Bayesian information criterion as \\[\\begin{equation} \\text{BIC} = -\\sum_{i=1}^n \\log f(\\hat{y}_i;\\hat{\\boldsymbol{\\mu}},\\hat{\\sigma}^2)+p\\log(n) + \\text{const.} \\end{equation}\\] where \\(\\text{const.}\\) does not depend on the model and hence can be ommitted when comparing models. The BIC is derived from Bayesian inference arguments, but is not related to information theory. Compared to the AIC (or indeed the \\(C_p\\)), the BIC uses \\(p\\log(n)\\) istead of \\(2p\\) as an estimated penalty and since \\(\\log(n)&gt; 2\\) for any \\(n &gt; 7\\), the BIC statistic generally places a heavier penalty on models with many variables, and hence results in the selection of smaller models than AIC. 2.5 Mean Squared Error Based Criteria 2.5.1 Stein’s unbiased risk estimator (SURE) The SURE can in principle by used to compare models. A standard application of SURE is in shrinkage estimators (see Section ????). (Stein 1981) derived an unbiased estimator of the mean-squared error showing that \\(\\mathbb{E}\\left[Zf(Z)\\right]=\\mathbb{E}\\left[f^{&#39;}(Z)\\right]\\) for \\(Z\\sim N(0,1)\\). Putting \\(Y_i\\sim N(\\mu_i,\\sigma^2)\\) and \\(\\hat{Y}_i=\\hat{\\mu}(Y_i)\\) we get \\[\\begin{equation} \\mathbb{E}\\left[(Y_i-\\mu_i)\\hat{\\mu}(Y_i)\\right]=\\text{cov}\\left[Y_i,\\hat{Y}_i\\right]=\\sigma^2\\mathbb{E}\\left[\\partial\\hat{\\mu}(Y_i)/\\partial Y_i\\right] \\end{equation}\\] The SURE is an unbiased estimator of \\(\\mathbb{E}\\left[\\vert\\vert \\mu-\\hat{\\mu}\\vert\\vert_2^2\\right]\\), given by \\[\\begin{equation} \\text{SURE}=-n\\sigma^2+\\sum_{i=1}^n\\left(\\hat{y}_i-y_i\\right)^2+2\\sigma^2\\sum_{i=1}^n \\partial\\hat{\\mu}(y_i)/\\partial y_i \\end{equation}\\] Note that SURE is not equal to the \\(C_p\\). Exercise: show that (1) \\(\\text{cov} \\left(\\hat{Y}_i;Y_i\\right)=\\sigma^2\\mathbb{E}\\left[\\partial\\hat{Y}_i/\\partial Y_i\\right]\\) and (2) an unbiased estimator of \\(\\mathbb{E}\\left[\\vert\\vert \\mu-\\hat{\\mu}\\vert\\vert_2^2\\right]\\) is given by (????) 2.5.2 The Focused Information Criterion (FIC) The FIC in its original format (see (???)) interprets best model in the sense of minimizing the mean squared error (MSE) of the estimator of the quantity of interest. The FIC philosophy puts less emphasis on which variables are in the model but rather on the accuracy of the estimator of a focus. To build the FIC, one considers a model of the form \\(F_{\\nu, \\gamma}\\), with density \\(f(\\cdot;\\nu,\\gamma)\\), with \\(\\nu \\in \\mathbb{R}^p\\) not subject to model selection (i.e. included in all considered models), \\(\\gamma \\in \\mathbb{R}^q\\) the parameters on which model selection is performed. \\(\\gamma\\) and \\(q\\) are allowed to depend on the sample size \\(n\\), hence \\(\\gamma:= \\gamma_{n}\\) and \\(q:= q_n\\). For the linear regression model \\(Y_i|\\mathbf{x}_i \\sim \\mathcal{N}(\\beta_0+\\mathbf{x}_i\\beta,\\sigma^2)\\)(with \\(\\mathbf{x}_i\\) not containing the one in the first column), a natural choice is \\(\\nu = (\\beta_0,\\sigma^2)\\) and \\(\\gamma_n = \\beta\\). The focus, or quantity of interest, is \\(\\hat{\\mu}:=\\mu(\\nu,\\gamma_n)\\), which can be, but not necessarily, the prediction for one particular new observation. Given a (chosen) estimator for the focus, \\(\\mu(\\hat{\\nu},\\hat{\\gamma_n})\\), assuming \\(\\gamma_n= \\gamma + \\delta/\\sqrt{n}\\) and considering a fixed value \\(q\\), (???) use a taylor expansion of \\(\\sqrt{n}\\left(\\hat{\\mu}-\\mu\\right)\\) to derive the bias and variance to build the (first order) MSE. More specifically, consider the set of indices \\(S\\subseteq \\left\\{1,\\ldots,q\\right\\}\\)such that \\(\\gamma_S\\subseteq \\gamma\\)is the corresponding subset of parameters, hence forming a submodel \\(S\\) of \\(F_{\\nu,\\gamma}\\), one gets \\[\\begin{equation} \\sqrt{n}\\left(\\hat{\\mu}_S-\\mu\\right)\\approx \\left[\\frac{\\partial\\mu(\\nu,\\gamma)}{\\partial\\nu}\\right]^T\\sqrt{n}\\left(\\hat{\\nu}-\\nu\\right)+\\left[\\frac{\\partial\\mu(\\nu,\\gamma)}{\\partial\\gamma_S}\\right]^T\\sqrt{n}\\left(\\hat{\\gamma}_S-\\gamma\\right)\\delta \\end{equation}\\] To derive the MSE, (???) use the asymptotic distribution of \\(\\sqrt{n}\\left(\\hat{\\mu}_S-\\mu\\right)\\) which, in the case of the MLE, assuming the correct model is \\(S\\) is For the linear regression model, (???) show that the MSE is then given by \\[\\begin{equation} \\mbox{MSE}_S=\\ldots \\end{equation}\\] 2.6 Classification measures Sensitivity: probability of predicting disease given true state is disease. Specificity: probability of predicting non-disease given true state is non- disease. ROC curves: https://en.wikipedia.org/wiki/Receiver_operating_characteristic The receiver operating characteristic curve (ROC) is a commonly used summary for assessing the tradeoff between sensitivity and specificity. It is a plot of the sensitivity versus specificity as we vary the parameters of a classification rule. The area under the ROC is a commonly used quantitative summary measure. It is sometimes called the c-statistic. However, for evaluating the contribution of an additional predictor when added to a standard model, the c-statistic may not be an informative measure. The new predictor can be very significant in terms of the change in model deviance, but show only a small increase in the c- statistic (see (???)). 2.7 Generalized measures Deviance based criteria and others References "],
["ordering-the-variables.html", "3 Ordering the variables 3.1 Introduction 3.2 Least squares stepwise and orthogonal matching pursuit 3.3 Classification And Regression Tree (CART)4", " 3 Ordering the variables 3.1 Introduction The criteria presented in Chapter 2 are out-of-sample measures of adequacy for a given model. Model selection consists in finding the best model (or a limited set), like the one optimizing the out-of-sample adequacy. This implies comparing the potential models that can be built given the available data. In high dimensional settings, the available set of potential predictors can be very large and an exhaustive building of potential models is just impossible. One hence needs to find suitable methods for ordering the predictors that enter the model sequentially, to constitute, at most, \\(\\huge p\\) potential models to be assessed and compared. In this chapter, we present some available ordering algorithms, which have the following form: Let \\(\\huge \\cal{M}_0\\) denote the null model, which typically contains no predictors. For \\(\\huge k = 0,\\ldots,p-1\\), do : Consider all \\(\\huge p-k\\) models that augment the predictors in \\(\\huge \\mathcal{M}_k\\) with one additional predictor. Choose the best among these \\(\\huge p-k\\) models, and call it \\(\\huge \\mathcal{M}_{k+1}\\). Stop the algorithm if \\(\\huge \\mathcal{M}_{k+1}\\) is not better than \\(\\huge \\mathcal{M}_{k}\\) and provide \\(\\huge \\mathcal{M}_{k}\\) as output. These algorithms are generally named stepwise forward algorithms and they differ in the definition of best in 2(b) and in the stopping rule criteria in 3. For the latter, it is actually chosen among the criteria presented in Chapter 2. Exercise: Compare the number of models to be considered in an exhaustive approach to the ones considered in a stepwise forward approach. Choose \\(\\huge p=\\{5,10,15,20\\}\\) and suppose that the true (best) model has \\(\\huge p\\) and \\(\\huge p/2\\) predictors. 3.2 Least squares stepwise and orthogonal matching pursuit 3.3 Classification And Regression Tree (CART)4 The regression model involves estimating the conditional mean of the response variable \\(\\huge Y\\) given a set of predictors \\(\\huge X_j, j=1,\\ldots,p\\) (collected in the \\(\\huge n\\times p\\) design matrix \\(\\huge \\mathbf{X}\\)), i.e. \\(\\huge \\mu(\\mathbf{X})=\\mathbf{X}\\beta, \\dim(\\beta)=p\\). The parameters \\(\\huge \\beta\\) represent the slopes of the linear regression of \\(\\huge Y\\) on \\(\\huge \\mathbf{X}\\). The basis of regression trees is to approximate these slopes by partitionning the observed predictors into consecutive sets of values, for which the observed mean response is computed. More precisely, tree-based methods partition the predictors’ space (feature space) into a set of rectangles, and then fit a simple model (i.e. computes the mean) in each one. The partitionning is done sequentially, with usually a binary partionning (i.e. in two consecutive parts), one predictor at the time. Figure shows such a sequence in the case of two predictors \\(\\huge X_1, X_2\\), with splitting values \\(\\huge t_m\\) and (splitted) regions \\(\\huge R_m, m=1,\\ldots,M, M=5\\). Note that the partition on the top left panel is not possible since one of the consequence of a recursive splitting is that each region is necessary included in only one larger region. Illustration of the tree method (taken from (???)) A regression tree algorithm has the following features (at each stage of the algorithm) that are problem dependent: criterion for the choice of the splitting variable \\(\\huge X_j\\) criterion for the splitting value \\(\\huge t_m\\) the model that is fitted in each region \\(\\huge R_m\\) the size of the tree \\(\\huge M\\) the topology (shape) the tree should have. 3.3.1 Regression tree In the linear regression model, since one is modelling a conditional mean, it makes sense to fit a constant model in each region \\(\\huge R_m\\), so that the fitted model at the mth stage of the algoritm is \\[\\begin{equation} \\huge \\hat{\\mu}(\\mathbf{X})= \\sum_{m=1}^M c_m(y_i\\vert \\mathbf{x}_i\\in R_m) \\end{equation}\\] with \\[\\begin{equation} \\huge c_m(y_i\\vert \\mathbf{x}_i\\in R_m)=\\frac{1}{n_m}\\sum_i^n y_i \\iota (\\mathbf{x}_i\\in R_m), n_m=\\sum_i^n \\iota (\\mathbf{x}_i\\in R_m) \\end{equation}\\] so that the residual sum of squares (RSS) \\(\\huge \\sum_{i=1}^n\\left(y_i-\\hat{\\mu}(\\mathbf{x}_i)\\right)^2\\) is minimized. At stage \\(\\huge m\\) of the algorithm, two new regions will be created (actually one of the former region will be split into two parts), \\(\\huge R_{m_1}(t_m)\\) and \\(\\huge R_{m_2}(t_m)\\) according to a splitting scalar \\(\\huge t_m\\) so that the RSS is minimized, i.e. \\[\\begin{eqnarray} \\huge \\min_{j,t_m}&amp; \\huge \\left\\{\\sum_{\\mathbf{x}_i \\in R_{m_1}(t_m)} \\left(y_i-c_m(y_i\\vert \\mathbf{x}_i\\in R_{m_1}(t_m))\\right)^2 \\right. \\\\ &amp; \\left. \\huge + \\sum_{\\mathbf{x}_i \\in R_{m_2}(t_m)} \\left(y_i-c_m(y_i\\vert \\mathbf{x}_i \\in R_{m_2}(t_m))\\right)^2\\right\\} \\end{eqnarray}\\] Hence, the splitting objective does only depend on the RSS of the two newly created regions (and not the others), which is a necessary simplification in order to avoid running into computationnally infeasible algorithm. For the size of the tree, there are several strategies. One approach would to stop splitting (or growing the tree) when the decrease in RSS (computed globally) due to the split is below some threshold. Another strategy is to set a priori the tree size \\(\\huge M\\) and then prun the tree using cost-complexity pruning. Let the subtree \\(\\huge T \\subset T_M\\) of the tree \\(\\huge T_M\\) obtained by fixing the size to \\(\\huge M\\), let also \\(\\huge \\vert T\\vert\\) be the size of \\(\\huge T\\) and \\(\\huge R_m(T), c_m(T)\\) defined accordingly as before, the cost complexity criterion is given by \\[\\begin{equation} \\huge C_{\\alpha}(T)=\\sum_{m=1}^{\\vert T\\vert}\\sum_{\\mathbf{x}_i\\in R_m(T)}\\left(y_i-c_m(T)\\right)^2 - \\alpha \\vert T\\vert \\end{equation}\\] Hence, the idea is to find, for each \\(\\huge \\alpha\\), the (unique) subtree \\(\\huge T_{\\alpha} \\subset T_M\\) that minimizes \\(\\huge C_{\\alpha}(T)\\). The tuning parameter \\(\\huge \\alpha \\geq 0\\) governs the tradeoff between tree size (to avoid overfitting) and its goodness of fit to the data. Large values of \\(\\huge \\alpha\\) result in smaller trees \\(\\huge T_{\\alpha}\\) (\\(\\huge \\alpha\\) corresponding to \\(\\huge T_M\\)). A sequence of subtrees can hence be constructed by letting \\(\\huge \\alpha\\) increase, and stop when a criterion such as tenfold cross-validation is optimized. Exercise: play with the rpart package in R (see also https://www.statmethods.net/advstats/cart.html) and a suitable dataset. 3.3.2 Classification Trees If the target is a classification outcome taking values \\(\\huge k=1, 2,\\ldots,K\\), the criteria for splitting (growing) and pruning the tree need to be adapted. Let \\(\\huge p_{mk}=1/n_m\\sum_{\\mathbf{x}_i\\in R_m}\\iota(y_i= k)\\) be the proportion of of class \\(\\huge k\\) observations in region \\(\\huge m\\). Different alternatives to the RSS (i.e. risk measures) exist and include the following: Misclassification error: \\(\\huge 1-p_{mk}\\) Gini index: \\(\\huge \\sum_{k=1}^Kp_{mk}(1-p_{mk})= \\sum_{k=1}^K\\sum_{k&#39;\\neq k}p_{mk}p_{mk&#39;}\\) Cross-entropy or deviance: \\(\\huge - \\sum_{k=1}^Kp_{mk}\\log(p_{mk})\\) When \\(\\huge K=2\\), one is in the binary case, which can be modelled as a logistic regression, i.e. \\(\\huge Y_i\\vert\\mathbf{x}_i\\sim B(1,\\pi(\\mathbf{x}_i))\\), with \\(\\huge \\mbox{logit}(\\pi(\\mathbf{x}_i))=\\mathbf{x}_i\\beta, \\dim(\\beta)=p\\), or \\[\\begin{equation} \\huge \\pi(\\mathbf{x}_i)=\\frac{\\exp(\\mathbf{x}_i\\beta)}{1+\\exp(\\mathbf{x}_i\\beta)} \\end{equation}\\] In that case, the deviance measure will correspond the the best fitted logistic regression. Figure: Put here a figure like FIGURE 9.3 in (???) with the caption *Alternative risk measures in the binary case, as a function of the proportion \\(\\huge \\pi\\). The deviance has been scaled to pass through \\(\\huge (0.5, 0.5)\\). The deviance and the Gini index have the advantage of being differentiable functions, and hence more amenable to numerical optimization. Exercise: write the CART algorithm (with pruniing) in the classification case. Exercise: play with the rpart package in R (see also https://www.statmethods.net/advstats/cart.html) and a suitable dataset. 3.3.3 Remarks CART methods provide prediction models \\(\\huge \\hat{\\mu}(\\mathbf{X})\\). They hence do not necessarily lead to parsimonious models, which, in terms of model interpretability, might be a serious problem. Are there ways around it? In classification problems, the consequences of misclassifying observations are more serious in some classes than others. For example, it is probably worse to predict that a person will not have a heart attack when he/she actually will, than vice versa. To account for this, one can define a \\(\\huge K \\times K\\) loss matrix \\(\\huge L\\), with \\(\\huge L_{kk&#39;}\\) being the loss incurred for classifying a class \\(\\huge k\\) observation as class \\(\\huge k&#39;\\), with obviously \\(\\huge L_{kk}=0, \\forall k\\). The Gini index could then be modified as \\(\\huge \\sum_{k=1}^K\\sum_{k&#39;\\neq k}L_{kk&#39;}p_{mk}p_{mk&#39;}\\) which has only an effect for \\(\\huge K&gt;2\\). In the binary case, a better approach is to directly weight the observations in class \\(\\huge k\\) by \\(\\huge L_{kk&#39;}\\) and use either the Gini index or the deviance. One major problem with trees is their high variance. Often a small change in the data can result in a very different series of splits, making interpre- tation somewhat precarious. Averaging methods such as bagging (see Section ???) can be used to reduce the variability of the tree estimator. 3.3.4 Others matching pursuit etc. See also (???), chapter 9.↩ "],
["shinkage-methods.html", "4 Shinkage Methods 4.1 Example one 4.2 Example two", " 4 Shinkage Methods Some significant applications are demonstrated in this chapter. 4.1 Example one 4.2 Example two "],
["smoothing-and-averaging.html", "5 Smoothing and averaging 5.1 Bagging", " 5 Smoothing and averaging 5.1 Bagging Bootstrap aggregation, or bagging, is a general-purpose procedure for reducing the bagging variance of a statistical learning method (especially with decision trees). See Introduction to statistical learning, section 8.2. "],
["extensions.html", "6 Extensions 6.1 PDC", " 6 Extensions 6.1 PDC Linear Model Generalized Linear Model "],
["references.html", "References", " References "]
]
